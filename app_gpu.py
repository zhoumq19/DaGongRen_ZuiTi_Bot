# å‘é‡æ¨¡å‹ä¸‹è½½
import os
from modelscope import snapshot_download
# å¯¼å…¥æ‰€éœ€çš„åº“
from typing import List
import numpy as np

import torch
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM


# æ¨¡å‹ä¸‹è½½åŠŸèƒ½,å¦‚æœæ–‡ä»¶å¤¹å·²ç»å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
def my_snapshot_download(model_name,cache_dir):
    model_dir = os.path.join(cache_dir, model_name.split('/')[-1])
    if not os.path.exists(model_dir):
        # å¦‚æœæ¨¡å‹æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œæ‰§è¡Œsnapshot download
        model_dir = snapshot_download(model_name, cache_dir=cache_dir)
    else:
        print(f"æ¨¡å‹æ–‡ä»¶å¤¹ {model_dir} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
        

cache_dir = '.'
model_name = "AI-ModelScope/bge-small-zh-v1.5"
model_dir=my_snapshot_download(model_name, cache_dir=cache_dir)
# æºå¤§æ¨¡å‹ä¸‹è½½
model_name = "IEITYuan/Yuan2-2B-July-hf"
model_dir = my_snapshot_download(model_name, cache_dir=cache_dir)

# å®šä¹‰å‘é‡æ¨¡å‹ç±»
class EmbeddingModel:
    """
    class for EmbeddingModel
    """

    def __init__(self, path: str) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(path)

        self.model = AutoModel.from_pretrained(path).cuda()
        print(f'Loading EmbeddingModel from {path}.')

    def get_embeddings(self, texts: List) -> List[float]:
        """
        calculate embedding for text list
        """
        encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
        encoded_input = {k: v.cuda() for k, v in encoded_input.items()}
        with torch.no_grad():
            model_output = self.model(**encoded_input)
            sentence_embeddings = model_output[0][:, 0]
        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
        return sentence_embeddings.tolist()
    
print("> Create embedding model...")
embed_model_path = './AI-ModelScope/bge-small-zh-v1___5'
embed_model = EmbeddingModel(embed_model_path)

# å®šä¹‰å‘é‡åº“ç´¢å¼•ç±»
class VectorStoreIndex:
    """
    class for VectorStoreIndex
    """

    def __init__(self, doecment_path: str, embed_model: EmbeddingModel) -> None:
        self.documents = []
        for line in open(doecment_path, 'r', encoding='utf-8'):
            line = line.strip()
            self.documents.append(line)

        self.embed_model = embed_model
        self.vectors = self.embed_model.get_embeddings(self.documents)

        print(f'Loading {len(self.documents)} documents for {doecment_path}.')

    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:
        """
        calculate cosine similarity between two vectors
        """
        dot_product = np.dot(vector1, vector2)
        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)
        if not magnitude:
            return 0
        return dot_product / magnitude

    def query(self, question: str, k: int = 1) -> List[str]:
        question_vector = self.embed_model.get_embeddings([question])[0]
        result = np.array([self.get_similarity(question_vector, vector) for vector in self.vectors])
        return np.array(self.documents)[result.argsort()[-k:][::-1]].tolist() 
    

# å®šä¹‰å¤§è¯­è¨€æ¨¡å‹ç±»
class LLM:
    """
    class for Yuan2.0 LLM
    """

    def __init__(self, model_path: str) -> None:
        print("Creat tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
        self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)

        print("Creat model...")
        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()

        print(f'Loading Yuan2.0 model from {model_path}.')

    def generate(self, question: str, context: List, temperature: float = 1.0):
        if context:
            prompt = f'èƒŒæ™¯ï¼š{context}\né—®é¢˜ï¼š{question}\nè¯·åŸºäºèƒŒæ™¯ï¼Œå›ç­”é—®é¢˜ã€‚ä¸è¶…è¿‡50ä¸ªå­—ã€‚'
        else:
            prompt = question

        prompt += "<sep>"
        inputs = self.tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
        # outputs = self.model.generate(inputs, do_sample=False, max_length=1024)
        outputs = self.model.generate(inputs,do_sample=True, max_length=1024,temperature=temperature)#, top_k=50, top_p=0.95)
        output = self.tokenizer.decode(outputs[0])

        # print(output.split("<sep>")[-1])
        
        return output.split("<sep>")[-1].split("<eod>")[0]
    

print("> Create Yuan2.0 LLM...")
# model_path = './IEITYuan/Yuan2-2B-Mars-hf'
model_path = './IEITYuan/Yuan2-2B-July-hf'
llm = LLM(model_path)

# å¾®ä¿¡çŠ¶æ€åˆ—è¡¨å’Œå¯¹åº”çŸ¥è¯†åº“
button_values = ['chill', 'crush', 'joy']
true_values=['æ¾å¼›ã€è‡ªåœ¨','æ‹çˆ±ã€å¿ƒåŠ¨','å¿«ä¹ã€ç¾æ»‹æ»‹']
print("> Create index...")
index_list=[]
for value in button_values:
    # doecment_path = './knowledge.txt'
    doecment_path=f'/mnt/workspace/æ‰“å·¥å˜´æ›¿æœºå™¨äºº/{value}.txt'
    index_list.append(VectorStoreIndex(doecment_path, embed_model))
    
# å¯¼å…¥å‰ç«¯å¼€å‘åº“
import streamlit as st

def main():
# if __name__ == '__main__':
    # åˆ›å»ºä¸€ä¸ªæ ‡é¢˜
    st.title('ğŸ’¬ æ‰“å·¥äººå˜´æ›¿æœºå™¨äºº') 
    st.caption("ğŸš€ A Streamlit chatbot powered by æµªæ½®ä¿¡æ¯Yuan2.0å¤§æ¨¡å‹")
    
    with st.chat_message("assistant"):
        st.write("å—¨ğŸ‘‹ï¼Œæˆ‘æ˜¯ä¸€ä¸ªè‹¦å‘½æ‰“å·¥äººï¼Œçœ‹æ¥ä½ æƒ³è®¾ç½®ä¸€ä¸ªå¾®ä¿¡çŠ¶æ€ï¼Œéœ€è¦ä¸€ä¸ªé…å¥—æ–‡æ¡ˆã€‚ä½ æƒ³è®¾ç½®ä¸€ä¸ªä»€ä¹ˆçŠ¶æ€ï¼Ÿ")
    
    
    for value,index,true_value in zip(button_values,index_list,true_values):
        if st.button(value):
            st.write(f"å¥½çš„è€æ¿ï¼Œæˆ‘å¸®ä½ æƒ³ä¸€ä¸ª {value}çš„æ–‡æ¡ˆ")
            
            prompt1=f"ç”Ÿæˆä¸€å¥å¸¦æœ‰'{true_value}'æƒ…ç»ªä»·å€¼çš„å¥å­ï¼Œå°‘äº10ä¸ªå­—ã€‚"
            output1=llm.generate(prompt1, [],temperature=1.2)

            print(prompt1)
            print(f"æ ¹æ®prompt1ç”Ÿæˆçš„ä¸{true_value}æœ‰å…³çš„ä¸­æ–‡å¥å­ï¼š\n{output1}")
            
            # çŸ¢é‡åŒ–è€Œä¸”å¯»æ‰¾åŒ¹é…
            context = index.query(output1)
            print(f"context{context}")
            
            prompt2=f"""
            è¯·å°†ä»¥ä¸‹å¥å­è¿›è¡Œç®€å•ä¿®æ”¹ï¼Œä½¿å…¶è¡¨è¾¾å‡ºæ˜æ˜¾çš„{true_value}æƒ…ç»ªï¼š
            åŸå¥ï¼š{context}
            åªè¾“å‡ºä¿®æ”¹åçš„å¥å­ã€‚
            """
            summary=llm.generate(prompt2,context,temperature=1.2)
            
            print(f"prompt2æœ€ç»ˆä¸ºï¼š\n{prompt2}\nè¾“å‡ºçš„ç»“æœæœ€ç»ˆä¸ºï¼š\n{summary}")
            
            st.chat_message("assistant").write(summary)
            
if __name__ == '__main__':
    main()